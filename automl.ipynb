{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment,ScriptRunConfig\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\n",
    "from train import clean_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'capstone_automl'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing cpu-cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cpu-cluster\")\n",
    "    \n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    \n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = TabularDatasetFactory.from_delimited_files(\"https://www.openml.org/data/get_csv/16826755/phpMYEkMl\")\n",
    "ds = remote_dataset.to_pandas_dataframe()\n",
    "x, y = clean_data(ds)\n",
    "training_data =  pd.concat([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"data\" not in os.listdir():\n",
    "    os.mkdir(\"./data\")\n",
    "    \n",
    "    \n",
    "training_data.to_csv('data/titanic.csv')\n",
    "\n",
    "# get the datastore to upload prepared data\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# upload the local file from src_dir to the target_path in datastore\n",
    "datastore.upload(src_dir='data', target_path='data')\n",
    "\n",
    "# create a dataset referencing the cloud location\n",
    "dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, ('data/titanic.csv'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"primary_metric\" : 'accuracy'\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(\n",
    "                             compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=dataset,\n",
    "                             label_column_name=\"survived\",   \n",
    "                             path = 'outputs',\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "remote_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run,model = remote_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "model_name = 'best_automl_model'\n",
    "\n",
    "# Initialize the ScoringExplainer\n",
    "scoring_explainer = TreeScoringExplainer(explainer.explainer, feature_maps=[automl_explainer_setup_obj.feature_map])\n",
    "\n",
    "# Pickle scoring explainer locally\n",
    "save(scoring_explainer, exist_ok=True)\n",
    "\n",
    "# Register trained automl model present in the 'outputs' folder in the artifacts\n",
    "original_model = best_run.register_model(model_name='automl_model', \n",
    "                                           model_path='outputs/model.pkl')\n",
    "\n",
    "# Register scoring explainer\n",
    "automl_run.upload_file('scoring_explainer.pkl', 'scoring_explainer.pkl')\n",
    "scoring_explainer_model = best_run.register_model(model_name='scoring_explainer', model_path='scoring_explainer.pkl')\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "\n",
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "myenv = Environment(name=\"myenv\")\n",
    "\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "azureml_pip_packages = [\n",
    "    'azureml-interpret', 'azureml-train-automl', 'azureml-defaults'\n",
    "]\n",
    "\n",
    "myenv = conda_dep.create(conda_packages=['scikit-learn', 'pandas', 'numpy', 'py-xgboost<=0.80'],\n",
    "                                 pip_packages=azureml_pip_packages,\n",
    "                                 pin_sdk_version=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deploy the model as a real-time endpoint\n",
    "\n",
    "endpoint_name = 'automlendpoint'\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,\n",
    "                                               memory_gb=1,\n",
    "                                               tags={\"data\": \"Titanic Data\",  \n",
    "                                                     \"method\" : \"local_explanation\"})\n",
    "                                             \n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score_local_explain.py\", environment=myenv)\n",
    "\n",
    "# Use configs and models generated above\n",
    "service = Model.deploy(ws,\n",
    "                       'model-scoring',\n",
    "                       [scoring_explainer_model, original_model],\n",
    "                       inference_config,\n",
    "                       aciconfig)\n",
    "                       \n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "swagger_uri = service.swagger_uri\n",
    "\n",
    "primary, secondary = service.get_keys()\n",
    "\n",
    "print(service.scoring_uri)\n",
    "\n",
    "print(service.swagger_uri)\n",
    "\n",
    "ConsumeEndPoint.SendRequest(scoring_uri,primaryKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "LogService.Log(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     compute_target.delete()\n",
    "# except ComputeTargetException:\n",
    "#     print(\"cpu-cluster Not Found\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}